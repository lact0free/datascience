---
title: "8. Data Cleaning 3"
author: "Ricardo"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_float: yes
    pandoc_args: ["--lua-filter=color-text.lua"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Load the packages
library(tidyverse)
library(tibble)
library(readr)
library(ggplot2)
library(dplyr)
library(stringr)
library(jsonlite)
library(purrr)
```

```{cat, engine.opts = list(file = "color-text.lua")}
Span = function(el)
  color = el.attributes['color']
  -- if no color attribute, return unchange
  if color == nil then return el end
  
  -- transform to <span style="color: red;"></span>
  if FORMAT:match 'html' then
    -- remove color attributes
    el.attributes['color'] = nil
    -- use style attribute instead
    el.attributes['style'] = 'color: ' .. color .. ';'
    -- return full span element
    return el
  elseif FORMAT:match 'latex' then
    -- remove color attributes
    el.attributes['color'] = nil
    -- encapsulate in latex code
    table.insert(
      el.content, 1,
      pandoc.RawInline('latex', '\\textcolor{'..color..'}{')
    )
    table.insert(
      el.content,
      pandoc.RawInline('latex', '}')
    )
    -- returns only span content
    return el.content
  else
    -- for other format return unchanged
    return el
  end
end
```

# **1. Maps and Anonymous Functions**

So far, we've learned how to use regular expressions to make cleaning and analyzing text data easier.

In our previous course, Intermediate R Programming, you learned why loops are not a good choice in R and why vectorization is a better alternative. In this lesson, we'll learn some tips and syntax shortcuts we can use, including:

Using map functions as a faster and simpler one-line code to replace loops.
Creating single use functions called anonymous functions.
The dataset we'll use in this lesson is in a format called JavaScript Object Notation (JSON). As the name indicates, JSON originated from the JavaScript language, but has now become a language-independent format. JSON is the main format used by servers to send data to webpages.

There are two types of objects in the JSON data:

- **List of values** It is identified by the square brackets, []. It is a set of values or objects separated by commas.
- **Collection of name/value pairs** We identify it with curly brackets, {}. Each name/value pair of the collection is in the following format "name" : value. Commas separate the pairs.


JSON data is built by nesting list of values and collection of name/value pairs as many times as desired. Here is an example of JSON:


![](https://dq-content.s3.amazonaws.com/401/r_json_type_1.svg)\

The JSON example above is a list containing three collections of name/value pairs. Each collection of name/value pairs has three name/value pairs, and the value of the third pair of each collection is itself a list.

From the R perspective:

- JSON list can be represented as R list.
- JSON Collection of name/value pairs can be represented as R dataframe.

![](https://dq-content.s3.amazonaws.com/401/r_json_type_r_1.svg)\


The R `jsonlite` package contains a number of functions to make working with JSON data easier. We can use the `fromJSON()` method to convert JSON data contained in a string to the equivalent set of R objects. Let see an example:

```{r}
json_string  <-  '
[
  {
    "name": "Sabine",
    "age": 36,
    "favorite_foods": ["Pumpkin", "Oatmeal"]
  },
  {
    "name": "Zoe",
    "age": 40,
    "favorite_foods": ["Chicken", "Pizza", "Chocolate"]
  },
  {
    "name": "Heidi",
    "age": 40,
    "favorite_foods": ["Caesar Salad"]
  }
]
'

json_df  <-  fromJSON(json_string)
print(class(json_df))
```


We can see that `json_string` has turned into a dataframe. Let's take a look at the structure of the dataframe using `str()` function:



```{r}
str(json_df)
```

We can see the formatting from our original string is gone. The main dataframe contains three variables and the `favorite_foods` variable is itself a list.

Let's now take a look at the values in the dataframe:


```{r}
print(json_df)
```

Here is the illustration of which transformations are taking place there:


![](https://dq-content.s3.amazonaws.com/401/r_json_transform.svg)\

Let's practice using fromJSON() to convert JSON data from a string to R dataframe!

**Instructions**

We have created a JSON string, `world_cup_str`, which contains data about games from the 2018 Football World Cup.

1. Import the `jsonlite` package.
2. Use `fromJSON()` to convert `world_cup_str` to a dataframe. Assign the result to `world_cup_df`.

```{r}
world_cup_str  <-  '
[
    {
        "team_1": "France",
        "team_2": "Croatia",
        "game_type": "Final",
        "score" : [4, 2]
    },
    {
        "team_1": "Belgium",
        "team_2": "England",
        "game_type": "3rd/4th Playoff",
        "score" : [2, 0]
    }
    ]
'

world_cup_df  <-  fromJSON(world_cup_str)
```

## Reading a JSON File

One of the places where the JSON format is commonly used is in the results returned by an **Application Programming Interface (API)**. APIs are interfaces that can be used to send and transmit data between different computer systems.

The dataset from this lesson — `hn_2014.json` — was downloaded from the Hacker News API. It's a different set of data from the CSV we've been using in the previous two lessons, and it contains data about stories from Hacker News in 2014.

To read a file from JSON format, we use the `fromJSON()` function. Note that this function is the same as the one used on the previous screen. However, instead of passing a string representing a JSON data as a parameter, the name of the JSON file is used. Let's look at how we would read that in our data:

```{r}

# Read in the JSON file
hn <- fromJSON("hn_2014.json") 

print(class(hn))
```


Our hn variable is a dataframe. Let's find out the structure of this dataframe, limiting the size of list to display to 2 because we cannot display all the dataframe. To do so, we will use `str()` function with `list.len` option:

```{r}
str(hn, list.len = 2)
```

Our dataset contains 35,806 observations and 10 variables, where each observation represents a Hacker News story. In order to understand the format of our dataset, we'll print the variables of the dataframe using the R built-in `colnames()` function:

```{r}
colnames(hn)
```


If we recall the dataset we used in the previous two lessons, we can see some similarities. There are variables representing the title, URL, points, number of comments, and date, as well as others that are less familiar. Here is a summary of the variables and the data they contain:

- `author`: The username of the person who submitted the story.
- `createdAt`: The date and time at which the story was created.
- `createdAtI`: An integer value representing the date and time at which the story was created.
- `numComments`: The number of comments that were made on the story.
- `objectId`: The unique identifier from Hacker News for the story.
- `points`: The number of points the story acquired, calculated as the total number of upvotes minus the total number of downvotes.
- `storyText`: The text of the story (if the story contains text).
- `tags`: A list of tags associated with the story.
- `title`: The title of the story.
- `url`: The URL that the story links to (if the story links to a URL).

Let's start by reading our Hacker News JSON file:

The JSON package remains imported from the previous screen.

1. Use the `fromJSON()` function to load the `hn_2014.json` file as a dataframe. Assign the result to hn.
2. From the console, try to explore `hn` by yourself to see what it contains.



## Deleting variables from a Dataframe


In data cleaning, the most important thing is to understand what data to clean. The analysis involves several explorations in order to find clues. On this screen, we will try to detect duplicates of variables and delete them.

Let's look at the first observation in full.

```{r}
# Retrieving the first row
first_story  <-  hn[1,]
str(first_story)
```
You may notice that the `createdAt` and `createdAtI` variables both have the date and time data in two different formats. Because the format of `createdAt` is much easier to understand, let's do some data cleaning by deleting the `createdAtI` variable from the dataframe.

One simple way of deleting a variable from a dataframe in R is to negate the name of this variable in the `select()` function: add the symbol - in front of the name of this variable. The `select()` function is from the `dplyr` package and you have already used it several times, mainly in the Data Cleaning in R course. Let's learn the syntax of deleting a variable by looking at a simple example:



```{r}
d  <-  data.frame('a' = c(1,3), 'b' = c("this", "that"), 'c'= c(2,4))

print(d)

#delete variable b
d  <-  d %>%
 select(-b)

print(d)
```



Let's use this technique to delete the `createdAtI` variable from first_story:




```{r}
first_story  <-  first_story %>%
                     select(-createdAtI)
str(first_story)
```

The dataframe returned by the function no longer includes the `createdAtI` variable.

Let's use the same technique to remove the `createdAtI` variable from every story in our Hacker News dataset:


We have provided the code `first_story` to inspire you. Our dataset, hn, is loaded from previous screens.

- Use the `select()` function to delete the `createdAtI` variable from the dataframe `hn`. Assign the cleaned dataframe to `hn_clean`.


```{r}
hn_clean <- hn %>% select(-createdAtI)

```

## Map Functions

In the previous lesson, Working With Vectorized Functions, you learned many R vectorized functions that are functions to quickly perform operations on vector's elements. However, all functions are not vectorized. For example, let's try to use `class()` function to display the classes of variables in our dataset `hn`.

```{r}
class(hn)
```


As you can see, the result is the class of the dataframe. It is not what we are expecting.

The problem is that the `class()` function does not support vectorization. There two ways to identify if a function support vectorization:

- Check the documentation of the function.
- Check manually, as we did here by running a quick code on it.

Let's now look at the result we have to obtain using a for-loop:

```{r}
for (name in colnames(hn)){
    print(class(hn[,name]))
}
```
As you may know, using loops in R is not really common. Hence, what if we want to achieve this result without the for-loop ?

To solve this problem, we can use a very powerful tool in R: map functions. These functions are used when we don't want to use a loop to execute a function on each element of a given vector or list or dataframe or matrix.

The good news is that we've already learned some of these functions. For example, the `map()` function of `purrr` package learned in lesson 2 of the Intermediate R Programming course.

This function is precisely the one we want to use here to solve our problem.

Let's use our demo JSON string to try and find the class of variables. First, we'll quickly remind ourselves of the data:



```{r}

json_string  <-  '
[
    {
        "name": "Sabine",
        "age": 36,
        "favorite_foods": ["Pumpkin", "Oatmeal"]
    },
    {
        "name": "Zoe",
        "age": 40,
        "favorite_foods": ["Chicken", "Pizza", "Chocolate"]
    },
    {
        "name": "Heidi",
        "age": 40,
        "favorite_foods": ["Caesar Salad"]
    }
]
'

json_df  <-  fromJSON(json_string)
print(json_df)
```


Let's try and use `map()` function to return a list of the classes:

```{r}

list_class <- map(json_df, class)

print(list_class)
```


As a reminder, the function `map()` requires two parameters:

1. The object on which the iterations have to be done.
2. The name of the function or a formula to use for each iteration.

The output of the `map()` function is a list. There are variations of the `map()` functional, which allow returning a vector in a specified data type.

- `map_df()` function outputs a dataframe.
- `map_lgl()` function outputs a logical vector.
- `map_int()` function outputs an integer vector.
- `map_dbl()` function outputs a double vector.
- `map_chr()` function outputs a character vector.


Those functions are used when we want to output a specific data type rather than a list.

Let's try the map_df() function which display the result as a dataframe/tibble.

```{r}
df_class <- map_df(json_df, class)
print(df_class)
```
Let's use the `map_df()` function to replace the for-loop to display the classes of variables in our dataset `hn`.

The dataframe `hn_clean` remains loaded from the previous screen.

Import the `purrr` package.

Create a variable `hn_classes` containaing the classes of each variable of the dataframe `hn_clean` using `map_df()` function.


```{r}
hn_classes <- map_df(hn_clean, class)
```

## Using Map Functions to Handle Our Dataframe

On the previous screen, we use map functions to display the class of our clean dataframe, hn_clean — our original dataset without the variable createdAtI. The result from the last screen is:

```{r}
print(hn_classes)
```

As you can see, the `tags` variable is a list. Let's use the `map_int()` function to show the length of each list in the `tags` variable. We'll use this map because we want to output integer vector. In R, you can use the built-in functions `length()` to display the length of a list and `unique()` to output only the distinct values from a vector.


```{r}
tags_length <- map_int(hn_clean[,'tags'], length)
print(unique(tags_length))
```

The `map_int()` function applies the `length()` function to each element of the `tags` variable and returns an integer vector. We use `unique()` to return only the distinct values of the resulting vector.

Now let's use `map_chr()` to look at the first element of each list in `tags` variable. The expecting output is a character vector which is why we are going to use `map_chr()`.


```{r}
first_tags <- map_chr(hn_clean[,'tags'], `[`, 1)
print(unique(first_tags))
```
We apply the` [ `function with an index of 1 to extract the first element of each list. The resulting character vector is then printed with the `unique()` function, which returns only the unique values of the vector.

Surprisingly, there is only a "story" string in the first element of each list. However, this is interesting information because it reveals that the first element of the list is not important.

Let's use the same technique to look at the second and third elements of each list in `tags` list.


The dataframe `hn_clean` and the `purrr` package remain loaded from previous screens. We have provided the tags list containing the `tags` from the `hn_clean` dataframe.

1. Create two variables, `second_tags` and `third_tags`, containing the second and third elements of each list in the `tags` variable, respectively.

- Ensure that these output vectors contain only the distinct values.


```{r}
tags <- hn_clean[,'tags']
second_tags <- unique(purrr::map_chr(tags, `[`, 2))
third_tags <- unique(purrr::map_chr(tags, `[`, 3))
```



In R programming, the `::` operator is used to refer to functions or objects from a specific package without needing to load the entire package into the current namespace.

For instance, if you have a function named `some_function()` that belongs to a package called `some_package`, you can access it using `::` without loading the entire `some_package.` The syntax is: `some_package::some_function()`.


## Anonymous Functions


Usually, we create functions when we want to perform the same task many times. On the two previous screens, we used map functions such as `map()`, `map_df()` and `map_chr()` to avoid using loops to make these repetitive tasks. We learned that these functions accept an object and a function name as parameter.

What if we wanted to use a custom function because the existing functions do not satisfy what we want to do? Let's say we wanted to check if the `favorite_foods` variable (from our demo JSON string) has lists of length 2 or not.

There are two ways to do so in R:

1. Write a separate function and use the name of this function, as was done for the R functions.
2. Use anonymous functions.

To learn this, let's use our demo JSON string:


```{r}
json_string  <-  '
[
    {
        "name": "Sabine",
        "age": 36,
        "favorite_foods": ["Pumpkin", "Oatmeal"]
    },
    {
        "name": "Zoe",
        "age": 40,
        "favorite_foods": ["Chicken", "Pizza", "Chocolate"]
    },
    {
        "name": "Heidi",
        "age": 40,
        "favorite_foods": ["Caesar Salad"]
    }
]
'

json_df  <-  fromJSON(json_string)

json_df
```



```{r}
food_lists  <-  json_df[,'favorite_foods']
```




We want to check which of the lists in `food_lists` have length 2. To do this, we will create the `check_foods_length()` function, which receives a list and output `"TRUE"` if this list is of length 2, and `"FALSE"` otherwise.

```{r}
check_foods_length <- function(x) {
     length(x) == 2
}
```


This function can be applied to any list.

```{r}
check_foods_length(c(1,2,3))
check_foods_length(c("this","that"))
```

We can use `map()` function to apply this function to each list of `favorite_foods.` Then, we can use the `map_lgl()` function to output a logical vector applying `check_foods_length()` function to each list of the `food_lists`:



```{r}
map_lgl(food_lists, check_foods_length)
```


R provides a special syntax to create temporary functions when:

- There is only one instruction in the functions
- We don't want to reuse them for anything else
These functions are called anonymous functions. Anonymous functions can be defined in a single line, which allows you to define a function you want to pass as an argument at the time you need it.

To create a **anonymous function** equivalent of our one-instruction `check_foods_length()` function, we removed:

- The name of the function
- The assignment symbol
- The brackets



![](https://dq-content.s3.amazonaws.com/401/anonymous_function_transform.svg)\

In the previous code, let's replace the `check_foods_length()` function by an anonymous function.

```{r}
map_lgl(food_lists, function(x) length(x) == 2)
```

Let's practice creating an anonymous function to check which of the lists in `food_lists` have length 1 or 3.\


In the display code, we have loaded the JSON data. We have provided with the `food_lists` variable.

1. Create a logical vector, `food_lists_logical` representing the output of applying an anonymous function to check which of the lists in `food_lists` have length 1 or 3.

- Build the anonymous function.
- Apply this anonymous function to food_lists using `map_lgl()` function.
- Assign the result to `food_lists_logical.`


```{r}
food_list_logical <- map_lgl(food_lists, function(x) length(x) == 1 | length(x) == 3)
```


## Using Anonymous Functions to Handle Our Dataframe



On the previous screen, we use Anonymous functions to check which of the lists in `food_lists` have length 2. With the same technique, we can check which of the lists in `tags_lists` from `hn_clean` dataset have length 4.


```{r}
tags_lists <- hn_clean[,'tags']
tags_lists_logical <- map_lgl(tags_lists, function(x) length(x) == 4)
```



Let's use `tags_lists_logical` to filter out the rows which don't have tag list of length 4. To do so we can use the function `filter()` from the `dplyr` package.



```{r}
hn_tags4 <- hn_clean %>%
    filter(tags_lists_logical)
str(hn_tags4[1,])
```

That is the first row of the new dataframe. There are 2,347 rows.

The two previous operations of finding which tag list is of length 4 and all the rows in `hn_clean` related to this can be done together, thanks to the power of `tidyverse.`


```{r}
hn_tags4 <- hn_clean %>%
    filter(map_lgl(tags, function(x) length(x) == 4))

str(hn_tags4[1,])
```


In the same way, let's create a new column in `hn_tags4` called, `point_labels`, that contains the value of the `points` variable followed by the string "pts".


```{r}
hn_tags4_new <- hn_tags4 %>%
    mutate(point_labels = map_chr(points, function(x) str_c(x,"pts")))

head(hn_tags4_new$point_labels)
```



In this code, we can notice the use of:

- The `str_c()` function from stringr package, which allows concatenating strings. For instance, `str_c("hello", "World")` outputs `"helloWorld"`. We taught this function in our Data cleaning in R course.
- The anonymous `function function(x) str_c(x,"pts")` to concatenate the values of points and "pts" string.
- The `map_chr()` to output a string vector.
- The `mutate()` function from `dplyr` package to create a new column.
- Let's use the same technique to create a new column containing the fourth tag in the `tags` variable from `hn_tags4` dataframe.




In the display code, we have loaded the `hn_tags4` dataframe for you. The packages `stringr`, `dplyr` and `purrr` are available from previous screens.

1. Create an anonymous function to index the fourth value of a list.
2. Create a new column, tag_4, containing the fourth value of each list in tags variable from `hn_tags4` dataframe.
3. Assign the result to the variable `hn_tags4_new`.


```{r}
hn_tags4 <- hn_clean %>%
    filter(map_lgl(tags, function(x) length(x) == 4))
hn_tags4_new <- hn_tags4 %>%
    mutate(tag_4 = map_chr(tags, function(x) x[4]))
```


##  Challenge: Cleaning Our Dataframe

The goal of this screen is to convert the dataframe of our JSON, embedding lists in the tags variable, into a simple dataframe containing only columns of atomic types — numeric, logical or character.

Considering our demo JSON string, here is the expected result:


```{r}
json_string  <-  '
[
    {
        "name": "Sabine",
        "age": 36,
        "favorite_foods": ["Pumpkin", "Oatmeal"]
    },
    {
        "name": "Zoe",
        "age": 40,
        "favorite_foods": ["Chicken", "Pizza", "Chocolate"]
    },
    {
        "name": "Heidi",
        "age": 40,
        "favorite_foods": ["Caesar Salad"]
    }
]
'
```

![](https://dq-content.s3.amazonaws.com/401/r_json_goal.svg)\


Here's how you can achieve this:

Read the JSON file using fromJSON() function from jsonlite package, which we just learned how to do. We will see the following for the JSON string:


```{r}
json_df <- fromJSON(json_string)
json_df
```

1. Try to unnest the JSON dataframe first. This operation consists of assigning each element of a list to a row by duplicating the other information in this row. To do this, there is a function with the same name,`unnest()`, in `tidyr` package. This function requires a parameter, `cols`, to specify the list of variables to which it will apply.


```{r}
# disentangle the tags from the list
json_df_unnest <- json_df %>%
                    unnest(cols = c(favorite_foods))
json_df_unnest
```


As we can see, the first row in `json_df` becomes two rows: one for `favorite_foods = Pumpkin`, and the other for the `favorite_foods = Oatmeal`.

2. Add a new column, `id_favorite_foods`, to the `json_df_unnest` dataframe to achieve the following result. This new column contains the string "favorite_foods_", followed by the row number for each group of duplicated rows.


```{r}
# create one extra column that maps each person
json_df_unnest <- json_df_unnest %>%  
  unnest(cols = c(favorite_foods)) %>% 
  group_by(name, age) %>% # group_by is used to make sure food can be allocated to each person
  mutate(id_favorite_foods = str_c("id_favorite_foods",row_number()))
  
json_df_unnest
```


3. Spread the `id_favorite_foods` and `favorite_foods` variables into new columns `favorite_foods_1`, `favorite_foods_2`, and `favorite_foods_3` to achieve our goal:

```{r}
json_df_unnest <- json_df_unnest %>% pivot_wider(names_from = id_favorite_foods,
                                                 values_from = favorite_foods)
print(json_df_unnest)
```


This screen is a challenge screen, so it's a little less guided than the previous exercises. We don't expect you to immediately figure this out, so don't be upset if this exercise takes you more attempts than the others.

If you get stuck, you might try one or more of the following:

- Use the test cases that we'll provide and try to follow the steps we've previously described.
- We've also provided a number of hints, however, we strongly recommend trying to first complete the challenge without them. The skills you build as you try to solve the challenge will be extremely valuable for you as you continue on your journey to becoming a data expert!


1. Use the provided JSON string and try to achieve all the above steps. Don't hesitate to look at the documentation and use the console to test cases.

2. Once you achieve the goal with the JSON string, try to replicate the code for `hn_clean_subset` dataset. Save the result as `hn_clean_unnest`.

Use the function `unnest()` from tidyr package to unnest the column tags of the dataframe `hn_clean_subset`.
Add a new column, id_tag, to the `hn_clean_subset` dataframe containing the string "tag_", followed by the row number for each group of duplicated rows.
Expand the id_tag and tags variables into new columns `tag_1`, `tag_2`, `tag_3`, and `tag_4` using `pivot_wider()` function from tidyr package as well.


```{r}
hn_clean_unnest <- hn_clean %>%
    unnest(cols = c(tags)) %>%
    group_by(author, numComments, points, url, storyText, createdAt, title, objectId) %>% 
    mutate(id_tag = str_c("tag_", row_number()))%>%
    pivot_wider(names_from = id_tag, 
                values_from = tags)
                                       
head(hn_clean_unnest)

```


# **2. Working with Missing Data**


## Introduction

In the last lesson of this course, we're going to learn more about working with missing data. We learned in the Dealing With Missing Data lesson from the Data Cleaning in R course:

What missing data is.
The various reasons data can be missing.
How to deal with missing data by either dropping rows and columns containing NA values or replacing them with non-NA values.
In this lesson, we'll cover additional techniques for handling missing data, including:

How to handle missing data without dropping rows and columns using imputation.
How to visualize missing data and missing data correlation matrices with heatmaps.
How to analyze missing data using visualization.
The dataset we'll use in this lesson is on motor vehicle collisions released by New York City (NYC) and published on the NYC OpenData website. It contains data on over 1.5 million collisions dating back to 2012, with additional data continuously added.

We'll work with an extract of the full data: Crashes from the year 2018. We made several modifications to the data for teaching purposes, including randomly sampling the data to reduce its size. You can download the dataset from this lesson by using the dataset preview tool at the top of the "script.r" codebox on the right.


```{r}
mvc  <-  read_csv("nypd_mvc_2018.csv")

head(mvc, 4)
```


A summary of the columns and their data is below:

- `unique_key`: A unique identifier for each collision.
- `date`, `time`: Date and time of the collision.
- `borough`: The borough, or area of New York City, where the collision occurred.
- `location`: Latitude and longitude coordinates for the collision.
- `on_street`, `cross_street`, `off_street`: Details of the street or intersection where the collision occurred.
- `pedestrians_injured`: Number of pedestrians who were injured.
- `cyclist_injured`: Number of people traveling on a bicycle who were injured.
- `motorist_injured`: Number of people traveling in a vehicle who were injured.
- `total_injured`: Total number of people injured.
- `pedestrians_killed`: Number of pedestrians who were killed.
- `cyclist_killed`: Number of people traveling on a bicycle who were killed.
- `motorist_killed`: Number of people traveling in a vehicle who were killed.
- `total_killed`: Total number of people killed.
- `vehicle_1` through `vehicle_5`: Type of each vehicle involved in the accident.
- `cause_vehicle_1` through `cause_vehicle_5`: Contributing factor for each vehicle in the accident.


Let's quickly recap how to check missing values. We'll start by creating a dataframe with some NA values, which represent missing data:

```{r}
df <- data.frame(A = c(NA,NA,1.0), B = c(NA,1,NA), C = c(1.0,1.0,NA))
print(df)
```
Next, we can use the is.na() function to identify which values are NA:

```{r}
is.na(df)
```



The output of `is.na()` function is a matrix containing logical values which indicate where NA values are by `TRUE` and `FALSE` otherwise.

Let's use this function to check the NA values in our dataset.

We have loaded the CSV in R called `mvc`. We have also loaded the `readr` package.

1. Create a matrix that counts the number of NA values in each of the columns in the `mvc` dataframe. Assign the result to `na_logical`.


```{r}
na_logical <- is.na(mvc)
```


## Summing Values Over Rows
On the previous screen, we loaded our dataset `mvc` and identified where missing values are using the `is.na()` function.

Let's quickly recap how to count missing values. To do so, we'll reuse the dataframe created on the first screen:

```{r}
df <- data.frame(A = c(NA,NA,1.0), B = c(NA,1,NA), C = c(1.0,1.0,NA))
print(df)
```



We can sum the output of the `is.na()` function using the built-in R function `colSums()`, to count the number of NA values in each column:


```{r}
colSums(is.na(df))
```


To refresh your memory, this function makes sums by column. It was introduced in the Dealing With Missing Data Lesson.

There is also the complement to this function which allows you to sum up the rows, `rowSums()` function. Here is an illustration of how it works :



![])(https://dq-content.s3.amazonaws.com/402/rowsums_colsums.svg)\

Let's compute how many missing values for each column using `rowSums()` function.


```{r}
rowSums(is.na(df))
```



Let's try to create new column, `ABsum`, containing the sums of rows from only variables `A` and `B`. To do so, we use the combination of `mutate()` function, from dplyr package, and `rowSums()` function.


```{r}
df_na  <- data.frame(is.na(df))

na_row_sums <- df_na %>%
    mutate(ABsum = rowSums(.[1:2]))

print(na_row_sums)
```

In this code, we use `period (.)` to represent all the dataset, and then we take its subset.

Let's use this technique to compute the total injured NA values from the NA values of the variables` pedestrians_injured`, `cyclist_injured`, `motorist_injured` in our dataset.


The dataframe `mvc` is available from the previous lesson.

1. Load the `dplyr` package.
2. Create a dataframe, `mvc_na`, containing logical values which indicate where missing values are.
3. Create a new column, `total_na_injured`, by summing the variables `pedestrians_injured`, `cyclist_injured`, `motorist_injured` from mvc_na dataframe.
Load the appropriate package to use pipe operator (`%>%`) and `mutate()` function.
Use rowSums to compute the number of NA values in these variables.
Use `mutate()` function to create the new column.
Assign the result `mvc_na_injured`.


```{r}
mvc_na <- data.frame(na_logical)

mvc_na_injured <- mvc_na %>% mutate(total_na_injured = rowSums(.[9:11])) #the period . is used to represent the whole dataset, followed by its subset [9:11]
```

## Verifying the Total Columns


To give us a better picture of the NA values in the data, let's calculate the percentage of NA values in each column. Below, we divide the number of NA values in each column by the total number of values in this column.

Let's first recall the code to count NA values in each column:

```{r}
# here is an example of how row and column sums work 
x <- cbind(x1 = 3, x2 = c(4:1, 2:5))
print(x)
rowSums(x) # sum up each row at once
colSums(x) # sum up each column at once
```

```{r}
na_counts  <-  mvc %>% is.na() %>% colSums()
data.frame(na_counts)
nrow(mvc)
```


Let's now compute the percentage of NA values in each column. The total number of values in a column is equal the number of rows in the dataset. We can use the built-in `nrow()` function to compute the number of rows.

```{r}
na_counts_pct  <-  na_counts * 100 / nrow(mvc)
```

We'll then add both the counts and percentages to a dataframe to make them easier to compare.

```{r}
na_df <- data.frame(na_counts = na_counts, na_pct = na_counts_pct )
# Rotate the dataframe so that rows become columns and vice-versa
na_df  <-  data.frame(t(na_df))

print(na_df)
```

It is worth noting that for the convenience of reading, we have rotated the dataframe. To do this, we combine the transposed function t() which always outputs a matrix and the `data.frame() `function to convert this matrix into a dataframe.



About a third of the columns have no NA values, with the rest ranging from less than 1% to 99%!

To make things easier, let's start by looking at the group of columns that relate to people killed in collisions.

We'll use the combination of `select()` with `ends_with()` functions from `dplyr` package, to reduce our summary dataframe to just those columns. The `ends_with()` function works only with `select()` function and allows selecting column names which end with a given string. In our dataset, the group of columns that relate to people killed in collisions ends with `"_killed"`.

```{r}
na_df_killed <- na_df %>% 
    select(ends_with("_killed"))
print(na_df_killed)
```

We can see that each of the individual categories have no missing values, but the total_killed column has five missing values.

One option for handling this would be to remove – or drop – those five rows. This would be a reasonably valid choice since it's a tiny portion of the data, but let's think about what other options we have first.

If you think about it, the total number of people killed should be the sum of each of the individual categories. We might be able to "fill in" the missing values with the sums of the individual columns for that row. The technical name for filling in a missing value with a replacement value is called imputation.

Let's look at how we could explore the values where the total_killed isn't equal to the sum of the other three columns. We'll illustrate this process using a series of diagrams. The diagrams won't contain values, they'll just show a grid to represent the values.

Let's start with a dataframe of just the four columns relating to people killed:



![](https://dq-content.s3.amazonaws.com/402/verify_totals_1_r.svg)\

We select just the first three columns, and manually sum each row:

![](https://dq-content.s3.amazonaws.com/402/verify_totals_2_r.svg)\


We then compare the manual sum to the original total column to create a logical vector where equivalent values are not equal:

![](https://dq-content.s3.amazonaws.com/402/verify_totals_3_r.svg)\

Lastly, we use the logical vector to filter the original dataframe to include only rows where the manual sum and original aren't equal:
![](https://dq-content.s3.amazonaws.com/402/verify_totals_4_r.svg)\


Let's use this strategy to look at the rows that don't match up!

We created a dataframe, `killed`, containing the four columns that relate to people `killed` in collisions. The package dplyr is available from previous screens.

1. Select the first three columns from `killed` and sum each row.

2. Assign the result to a new column `manual_sum` using the `mutate()` function.
Filter the `killed` rows where each value in the `manual_sum` column either is not equal to the values in the `total_killed` column or the `total_killed` column is `NA` value.

3. Assign the resulting dataframe to `killed_non_eq.`


```{r}
killed  <-  mvc %>% select(ends_with("_killed"))
killed_non_eq <- killed %>% 
    mutate(manual_sum = rowSums(.[1:3])) %>%
    filter(manual_sum != total_killed | is.na(total_killed))
```


## Filling and Verifying the Killed and Injured Data

The `killed_non_eq` dataframe we created in the previous exercise contained six rows with discrepancies in their `total_killed` values. We categorized these into two categories: - Five rows where the `total_killed` is not equal to the sum of the other columns because the total value is missing (NA). - One row where the `total_killed` is less than the sum of the other columns.]{color="red"}


We concluded that filling NA values with the sum of the columns is a good choice for our imputation, given that only six rows out of around 58,000 don't match this pattern.

We've also identified a row with suspicious data - one that doesn't sum correctly. After imputing the missing values for `total_killed`, we'll mark this suspect row by setting its value to NA.

We'll use a combination of the `mutate()` and `if_else()` functions from the dplyr package to accomplish this. This combination is useful when you want to replace certain values in a vector based on a condition.

Here's the syntax for this combination:

```{r}
# df %>% mutate(colname = if_else(condition, val_if_true, val_if_false))
```

Let's look at an example with some simple data. We'll start with a tibble called fruits:


```{r}
fruits <- tibble(name = c("Apple", "Banana", "Banana"))
```


![](https://dq-content.s3.amazonaws.com/402/mask_r_1.svg)\

Next, we create a logical vector that matches values equal to the string Banana:

```{r}
banana_check <- fruits$name == "Banana"
```



![](https://dq-content.s3.amazonaws.com/402/mask_r_2.svg)\

Now, we use the combination of mutate() and if_else() functions to replace all the values that match the logical vector with a new value, Pear:


```{r}
fruits <- fruits %>%
    mutate(name = if_else(name=="Banana", "Pear", name ))
```

![](https://dq-content.s3.amazonaws.com/402/mask_r_3.svg)\

The logic of the code above is: For each value in the "fruits" tibble, if the corresponding value in the logical vector is TRUE, update the value to "Pear," otherwise leave the original value.

In the first example, we updated a single value, but we can also update the matching values using a vector with the same length, like this nums vector:

```{r}
nums <- c("one", "two", "three")
```



![](https://dq-content.s3.amazonaws.com/402/mask_r_4.svg)\

Now, let's see how we can update the matching values in the fruits tibble with the corresponding values in the nums vector:


```{r}
fruits <- fruits %>%
    mutate(name = if_else(name=="Banana", nums, name ))
```



![](https://dq-content.s3.amazonaws.com/402/mask_r_5.svg)\


The logic of the code above is: For each value in the "fruits" tibble, if the corresponding value in the logical vector is TRUE, update the value to the corresponding value from the "nums" vector, otherwise leave the original value.

[To update the values in the total_killed column, first, we'll replace all NA values with the equivalent values from our manual_sum variable:]{color="red"}


```{r}
killed_non_eq <- killed_non_eq %>%
    mutate(total_killed = if_else(
        is.na(total_killed), 
        manual_sum, 
        total_killed 
      )
    )
```



[Next, we'll replace any values where the manual sum and the total column aren't equal with `NaN`:]{color="red'}

```{r}
killed_non_eq <- killed_non_eq %>%
    mutate(total_killed = if_else(
        total_killed != manual_sum, 
        NaN, 
        total_killed 
      )
    )
```


Now let's look at the values we've changed:

```{r}
print(killed_non_eq)
```

We've gone from five NA values to one, and flagged some suspicious data. Let's do the same for the injured columns.

We have provided the code to clean the `killed_non_eq` columns. Now, you need to clean the `injured_non_eq` dataframe, which contains just the `injured` columns and `manual_sum`, a column manually summing the three individual `injured` columns. The `dplyr` package is available from previous screens.

1. Use the combination of the `mutate()` and `if_else()` functions to replace any NA values in the `total_injured` column with their corresponding values from the `manual_sum` column.
2. Use the same combination to replace any values in the `total_injured` column that aren't equal to their corresponding values in the `manual_sum` column with `NaN`.


```{r}
# Create an injured_non_eq dataframe and manually sum values
injured  <-  mvc %>% select(ends_with("_injured"))

injured_non_eq <- injured %>% 
    mutate(manual_sum = rowSums(.[1:3])) %>%
    filter(manual_sum != total_injured | is.na(total_injured))
injured_non_eq <- injured_non_eq %>%
    mutate(total_injured = if_else(is.na(total_injured), manual_sum, total_injured ))
injured_non_eq <- injured_non_eq %>%
    mutate(total_injured = if_else(total_injured != manual_sum, NaN, total_injured ))
```



## Preparing Data for Missing Data Visualization


Earlier, we used a table of numbers to understand the number of missing values in our dataframe. Another approach we can take is to use a plot to visualize the missing values.

First, let's prepare our data. Since we want to work with missing data, we'll transform our dataset, mvc, into a dataframe where values are `1` if they are NA values and `0` otherwise. To do this, we will use the `is.na()` function to detect missing values, and then use the `as.numeric()` function to convert the logical results to numeric values. We will apply these functions to each column using the `map_df()` function from the `purrr` package, along with an anonymous function.

```{r}
mvc_na <- map_df(mvc, function(x) as.numeric(is.na(x)))
```


One of the most suitable visualizations for missing data is heatmaps. However, our data is not currently in a format compatible with this type of visualization. We need to reshape the data to have three variables: `x`, `y`, and `value`, where:

- [`x` represents the x-axis and contains the column name from our current dataset.]{color="red"}
- [`y` represents the y-axis and contains the row number from our current dataset.]{color="red"}
- `value` represents values from our current dataset flattened as one column, indicating whether each entry is missing (1) or not missing (0).


Let's assume we have the following dataframe:

```{r}
df  <-  data.frame('A' = c(1,2,3,4,5), 'B' = c(2,4,6,8,10), 'C'= c(3,6,9,12,15))
print(df)
```
Let's look at an example of what is expecting from df:
```{r}
# x    y      value
#  1 A     1      1.00
#  2 A     2      2.00
#  3 A     3      3.00
#  4 A     4      4.00
#  5 A     5      5.00
#  6 B     1      2.00
#  7 B     2      4.00
#  8 B     3      6.00
#  9 B     4      8.00
# 10 B     5      10.0 
# 11 C     1      3.00
# 12 C     2      6.00
# 13 C     3      9.00
# 14 C     4      12.0 
# 15 C     5      15.0
```

In R, we can obtain this result by using the `pivot_longer()` function to transform the dataframe into key-value pairs and then adding row numbers using the `row_number()` function. You can refresh your memory about the `pivot_longer()` function by looking at our Lesson Correlations and Reshaping Data.


```{r}
df_heat <- df %>%
  pivot_longer(cols = everything(),
               names_to = "x") %>%
  group_by(x) %>%
  mutate(y = row_number())

print(df_heat)
```


Notice how we use the `group_by()` function to number rows by column in the code below.[ We also use the function `everything()` to indicate that all columns are involved in the reshaping process. The `everything()` function is a "helper function" for selecting variables that should be included as the argument to the `cols` parameter.]{color="blue"} Using this function is equivalent to listing the names of all variables in the dataframe for the `cols` parameter.

Let's use the same technique to prepare our dataset.

We have provided the `mvc_na` dataframe. The `dplyr` package is available from previous screens.

1. Use the `pivot_longer()` function from the `tidyr` package to transform the `mvc_na` dataframe into a new dataframe containing `x` and `value` variables.
- Use the `everything() function to indicate that all columns are involved in the reshaping process.
2. Create a new column `y` containing the row numbers.
3. Assign the resulting dataframe to `mvc_na_heat`.


```{r}
mvc_na <- map_df(mvc, function(x) as.numeric(is.na(x)))
library(tidyr)
                 
mvc_na_heat <- mvc_na %>%
    pivot_longer(cols = everything(),
               names_to = "x") %>%
    group_by(x) %>%
    mutate(y = row_number())
```

## Visualizing Missing Data with Heatmaps

In the previous exercise, we prepared the mvc_na_heat dataframe for visualization. Let's look at the first rows of this dataframe:

```{r}
head(mvc_na_heat, 10)
```

Now, we'll create a helper function to generate heatmaps for visualizing missing data. The function below uses the `geom_tile()` function from the `ggplot2` package to represent NA values as light squares and non-NA values as dark squares:

```{r}
plot_na_matrix <- function(df) {
    # Preparing the dataframe for heatmaps 
    df_heat <- df %>%
        pivot_longer(cols = everything(),
               names_to = "x") %>%
        group_by(x) %>%
        mutate(y = row_number())

    # Ensuring the order of columns is kept as it is
    df_heat <- df_heat %>%
        ungroup() %>%
        mutate(x = factor(x,levels = colnames(df)))

    # Plotting data
    g <- ggplot(data = df_heat, aes(x=x, y=y, fill=value)) + 
        geom_tile() + 
        theme(legend.position = "none",
              axis.title.y=element_blank(),
              axis.text.y =element_blank(),
              axis.ticks.y=element_blank(),
              axis.title.x=element_blank(),
              axis.text.x = element_text(angle = 90, hjust = 1))

    # Returning the plot
    g
}
```



This helper function, `plot_na_matrix`, takes a dataframe `df` as input and performs the following steps:

- The `pivot_longer()` function is used to transform `df` into a key-value pair dataframe with columns `x` and `value.` The `group_by()` function then groups the data by the `x` column.
- The `mutate()` function is used to add a new column `y` containing row numbers.
- The `ungroup()` function is called to remove the grouping, and the `mutate()` function is used again to set the factor levels for the `x` column, ensuring that the order of columns in the plot matches their original order in the dataframe.
- The `ggplot()` function is used to create the plot, with `geom_tile()` creating a grid of squares. The `fill` aesthetic is set to the `value` column, so that each square is filled according to the corresponding value (NA or non-NA).
- The `theme()` function is used to customize the plot appearance, removing the legend and axis titles, and rotating the x-axis text labels.


[The final plot is a heatmap where light squares represent missing data (NA values), and dark squares represent non-missing data (non-NA values).]{color="red"} This visualization makes it easy to identify patterns and trends in the missing data across the dataframe.

Let's look at how the function works by using it to plot just the first row of our `mvc` dataframe. We'll display the first row as a table immediately below so it's easy to compare:

```{r}
plot_na_matrix(mvc_na[1, ])
```


Each value is represented by a dark square, and each missing value is represented by a light square.

Let's look at what a plot matrix looks like for the whole dataframe:

```{r}
plot_na_matrix(mvc_na)
```
We can make some immediate interpretations about our dataframe:

- The first three columns have few to no missing values.
- The next five columns have missing values scattered throughout, with each column seeming to have its own density of missing values.
- The next eight columns are the `injury` and `killed` columns we just learned how to clean.
- The last 10 columns seem to break into two groups of five, with each group of five having similar patterns of NA/non-NA values.
Let's visualize the missing data in the dataframe of vehicle variables.

We have provided the `plot_na_matrix()` function. `mvc_na` dataframe is available from previous screens.

1. Import the `ggplot2` package.
2. Create the subset of `mvc_na` dataframe where column names contain `"vehicle"`.
3. Use the `plot_na_matrix()` function to visualize this subset.

```{r}
mvc_na_vehicle <- mvc_na %>% select(contains("vehicle"))
plot_na_matrix(mvc_na_vehicle)
```

## Visualizing Correlation Matrix with Heatmaps

On the previous screen, we visualized missing values in our dataset mvc_na_heat. We realized that the last 10 columns seem to break into two groups of five, with each group of five having similar patterns of NA/non-NA values.

Let's examine the pattern in these last 10 columns a little more closely. We can calculate the relationship between two sets of columns, known as correlation. To calculate this, we use the cor() function (You can review correlation in our Correlations and Reshaping Data Lesson). Here's what that looks like:

```{r}
cols_with_missing_vals  <-  colnames(mvc_na)[colSums(mvc_na)> 0]
print(cols_with_missing_vals)
missing_corr  <-  round(cor(mvc_na[cols_with_missing_vals]),6) # 6 indicates the number of decimal places are kept in the function round
head(missing_corr)
```
Each value is between 
-1 and 1, and represents the relationship between two columns. A number close to -1 or 1
represents a strong relationship, whereas a number in the middle (close to0) represents a weak relationship.

If you look closely, you can see a diagonal line of 1s going from top left to bottom right. These values represent each column's relationship with itself, which of course is a perfect relationship. The values on the top/right of this "line of 1s" mirror the values on the bottom/left of this line: The table actually repeats every value twice!

Correlation tables can be hard to interpret. We can convert our table into a heatmap, which will make this a lot easier.

In this R code, we have a helper function named `plot_na_correlation` that takes a single argument df. The purpose of this function is to visualize the correlation between missing values in the dataset using a heatmap. Let's break down the code step by step to understand what's happening:

1. **Taking the lower triangle of the correlation matrix:** This step creates a copy of the input dataframe named `missing_corr_up` and sets the values of the lower triangle to NA (missing). This is done because we are only interested in the upper triangle of the correlation matrix to avoid redundancy in the heatmap.

2. **Preparing the dataframe for heatmaps**: This step converts the correlation matrix into a long format dataframe suitable for creating a heatmap. The `pivot_longer() `function from the `tidyverse` package is used to accomplish this.

3. **Ordering triangle:* *Here, we order the columns of the dataframe based on the number of missing values. We create two ordered column sets, one in ascending order and another in descending order.
4. **Plotting heatmaps:** In this step, we create a heatmap using the ggplot2 package. We use the `geom_tile()` function to create a heatmap, and `geom_text()` to add the correlation values as text labels on each tile. We also set the color gradient for the heatmap, ranging from white (for -1) to red (for 1), with yellow (for 0) in between.
```{r}
#plot na correlation helper
plot_na_correlation <- function(df) {
    # Taking the lower triangle of the correlation matrix
    missing_corr_up <- df
    missing_corr_up[lower.tri(missing_corr_up)] <- NA
    missing_corr_up <- data.frame(missing_corr_up)
    col_names <- colnames(missing_corr_up)
# Preparing the dataframe for heatmaps 
    missing_corr_up_heat <- missing_corr_up %>%
        pivot_longer(cols = everything(),
               names_to = "x") %>%
        group_by(x) %>%
        mutate(y = col_names[row_number()])  %>%
        na.omit
    # Ordering triangle
    ordered_cols_asc <- col_names[order(colSums(is.na(missing_corr_up)))]
    ordered_cols_desc <- col_names[order(-colSums(is.na(missing_corr_up)))]

    missing_corr_up_heat <- missing_corr_up_heat %>%
        ungroup() %>%
        mutate(x = factor(x,levels = ordered_cols_asc)) %>%
        mutate(y = factor(y,levels = ordered_cols_desc))
    
  # Plotting heatmaps
    g <- ggplot(data = missing_corr_up_heat, aes(x=x, y=y, fill=value)) + 
        geom_tile() + 
        geom_text(aes(label=value), size = 1.5) +
        theme_minimal() +
        scale_fill_gradientn(colours = c("white", "yellow", "red"), values = c(-1,0,1)) +
        theme(legend.position = "none",
              axis.title.y=element_blank(),
              axis.title.x=element_blank(),
              axis.text.x = element_text(angle = 90, hjust = 1))

    # Returning the plot
    g
}
#plot using plot_na_correlation
plot_na_correlation(missing_corr)
```

In our correlation plot:

- Values very close to 0, where there is little to no relationship, are in orange color.
- Values close to 1 are red and values close to -1 are yellow — the depth of color represents the strength of the relationship.
We provided a helper function to create correlation plots. Let's create a correlation plot of just those last 10 columns to see if we can more closely identify the pattern we saw earlier in the matrix plot.


We have provided the `plot_na_correlation()` function, which will plot correlations between NA values in a dataframe. `ggplot2` and `stringr` packages are available from the previous screen.

1. Use the `contains()` function to select columns from the `mvc_na` dataframe that contain the substring `'vehicle'`.
2. Create the correlations matrix using the `cor()` function and round the result to 2 digits after the decimal point. Assign the result as `missing_vehicle_corr`.
3. Pass missing_vehicle_corr to the `plot_na_correlation()` function.




```{r}
#plot na correlation helper
plot_na_correlation <- function(df) {
    # Taking the lower triangle of the correlation matrix
    missing_corr_up <- df
    missing_corr_up[lower.tri(missing_corr_up)] <- NA
    missing_corr_up <- data.frame(missing_corr_up)
    
    # Preparing the dataframe for heatmaps 
    col_names <- colnames(missing_corr_up)
    
    missing_corr_up_heat <- missing_corr_up %>%
        pivot_longer(cols = everything(),
               names_to = "x") %>%
        group_by(x) %>%
        mutate(y = col_names[row_number()])  %>%
        na.omit
    
    # Ordering triangle
    ordered_cols_asc <- col_names[order(colSums(is.na(missing_corr_up)))]
    ordered_cols_desc <- col_names[order(-colSums(is.na(missing_corr_up)))]
    
    missing_corr_up_heat <- missing_corr_up_heat %>%
        ungroup() %>%
        mutate(x = factor(x,levels = ordered_cols_asc)) %>%
        mutate(y = factor(y,levels = ordered_cols_desc))
    
    # Plotting heatmaps
    g <- ggplot(data = missing_corr_up_heat, aes(x=x, y=y, fill=value)) + 
        geom_tile() + 
        geom_text(aes(label=value), size = 2) +
        theme_minimal() +
        scale_fill_gradientn(colours = c("white", "yellow", "red"), values = c(-1,0,1)) +
        theme(legend.position = "none",
              axis.title.y=element_blank(),
              axis.title.x=element_blank(),
              axis.text.x = element_text(angle = 90, hjust = 1))
    
    # Returning the plot
    g
}
mvc_na_vehicle <- mvc_na %>% select(contains("vehicle"))
missing_vehicle_corr  <-  round(cor(mvc_na_vehicle), 2)

plot_na_correlation(missing_vehicle_corr)
```
## Analyzing Correlations in Missing Data

The plot you produced on the previous screen is below:
![](https://dq-content.s3.amazonaws.com/402/plot_correlations_vehicles_1bis.png)\

We outlined a diagonal strip of five squares in green that have a higher correlation than the rest. The pairs of column names that make up these five correlations are:

1. `vehicle_1` and `cause_vehicle_1`
2. `vehicle_2` and `cause_vehicle_2`
3. `vehicle_3` and `cause_vehicle_3`
4. `vehicle_4` and `cause_vehicle_4`
5. `vehicle_5` and `cause_vehicle_5`
If you think about it, this makes sense. When a vehicle is in an accident, there is likely to be a cause, and vice-versa.

Let's explore the variations in missing values from these five pairs of columns. We'll create a dataframe that counts, for each pair:

- The number of values where the vehicle is missing when the cause is not missing.
- The number of values where the cause is missing when the vehicle is not missing.

We have provided: A function `v_fun() `to count the number of rows where the column `vehicle_x` is NA and the column `cause_vehicle_x` is not NA (x is the number of vehicle and cause vehicle). A variable `v_na` containing the number of vehicle missing using `map_int()` and `v_fun()` functions.

1. Do the same for `cause`: Create a variable `c_na` containing the number of cause missing.
- Write a function `c_fun()` to count the number of rows where the column `vehicle_x` is not NA and the column `cause_vehicle_x` is NA (x is the number of vehicle and cause vehicle).
- Use `map_int()` and `c_fun()` functions to compute the number of cause missing.
- Assign the result to `c_na`.
2. Create a tibble `vc_na_df` containing, in order: v, `v_na`, and `c_na`.
- Set the tibble column names to the provided vector, `col_labels`.



```{r}
col_labels  <-  c('v_number', 'vehicle_missing', 'cause_missing')

v_fun <- function(x){ 
    v_col <- paste('vehicle', x,  sep = "_" )
    c_col <- paste('cause_vehicle', x,  sep = "_" )
    sum(is.na(mvc[v_col]) & !is.na(mvc[c_col]))
}
v_na <- map_int(1:5, v_fun )
c_fun <- function(x){ 
    v_col <- paste('vehicle', x,  sep = "_" )
    c_col <- paste('cause_vehicle', x,  sep = "_" )
    sum(!is.na(mvc[v_col]) & is.na(mvc[c_col]))
}
c_na <- map_int(1:5, c_fun )

vc_na_df  <-  tibble(1:5, v_na, c_na)
colnames(vc_na_df) <- col_labels
print(vc_na_df)
```


##  Finding the Most Common Values Across Multiple Columns

The analysis we did on the previous screen indicates that there are roughly 4,500 missing values across the 10 columns. The easiest option for handling these would be to drop the rows with missing values. This would mean losing almost 10% of the total data (4500 rows), which is something we ideally want to avoid.

A better option is to impute the data, like we did earlier. Because the data in these columns is text data (categorical variables), we can't perform a numeric calculation (like mean or median) to impute missing data like we did with the `injuries` and `killed` columns.

One common option when imputing is to use the most common value to fill in data. Let's look at the common values across these columns and see if we can use that to make a decision.

We've previously used the `table() `function to find the most common values in a single column. In this case, we want to find the most common values across multiple columns. In order to do this, we first need to convert our dataframe of multiple columns into one single column, and then we can use `table()` to count the items.

To convert a dataframe to a single column of values, we use the `pivot_longer()` function, which stacks a dataframe column into a key-value pairs dataframe object. For example, if we have a dataframe with columns A, B, and C, `pivot_longer()` can be used to create a new dataframe with two columns: one for the column names (A, B, or C) and one for the corresponding values.

Let's combine the `pivot_longer()` and `table()` functions to count the most common values for the cause set of columns. We'll start by selecting only the columns containing the substring cause. The `starts_with()` function is similar to the `ends_with()` function that we have used recently, and it selects columns whose names start with a specified string.




```{r}
cause  <-  mvc %>% select(starts_with("cause_"))

head(cause)
```
Next, we'll stack the values into a dataframe containing `name` and `value` variables:


```{r}
cause_1d <- cause %>% pivot_longer(cols = everything())
head(cause_1d)
```

Finally, we count the values in the vectors and output the most common non-NA values.


```{r}
cause_counts  <-  table(cause_1d$value)
print(cause_counts)
top_10_causes  <-  head(sort(cause_counts, decreasing = T), 10)
print(top_10_causes)
```

The most common non-NA value for the cause columns is `Unspecified`, which presumably indicates that the officer reporting the collision was unable to determine the cause for that vehicle.

Let's use the same technique to identify the most common non-NA value for the vehicle columns. We can use the identified most common values for each column to fill in the missing data, thus preserving as much data as possible and reducing the impact of missing information on our analysis.

We provided a dataframe — `vehicles` — that contains only the columns from `mvc` starting with `"vehicle_"`.

- Use `pivot_longer()` to stack the values from the `vehicles` dataframe into a key-value dataframe.
- Use `table()`to count the unique values from the stacked list. Assign the first 10 values to `top10_vehicles`.



```{r}
vehicles  <-  mvc %>% select(starts_with("vehicle_"))
head(vehicles)

vehicles_1d  <-  vehicles %>% pivot_longer(cols = everything())

vehicles_counts  <-  table(vehicles_1d$value)

head(vehicles_counts, 50)

top_10_vehicles  <-  head(sort(vehicles_counts, decreasing = T), 10)

print(top_10_vehicles)
```




## Filling Unknown Values with a Placeholder

Let's look at the values analysis we completed on the previous screen:


```{r}
print(top_10_causes)
print(top_10_vehicles)
```
The top "cause" is an "Unspecified" placeholder, which is more informative than an NA value because it distinguishes between a value that is missing due to a limited number of vehicles in the collision and one that is missing because the contributing cause for a particular vehicle is unknown.

The vehicles columns don't have an equivalent placeholder, but we can still use the same technique. Here's the logic we'll need to apply for each pair of vehicle/cause columns:

- For values where the vehicle is NA and the cause is non-NA, set the vehicle to `Unspecified`.
- For values where the cause is NA and the vehicle is non-NA, set the cause to `Unspecified`.
We can use the functions `mutate_at()` and `if_else()` to replace the values. Let's look at code to perform this for the `vehicle_1` and `cause_vehicle_1` columns using `mutate_at()`:


- Creates a logical vector for values where the vehicle column is NA and the cause column is non-NA.
- Creates a logical vector for values where the cause column is NA and the vehicle column is non-NA.
- Uses the first logical vector to fill matching values from the vehicle column with the string `Unspecified`.
- Uses the second logical vector to fill matching values from the cause column with the string `Unspecified`.
- Outside the loop, use the `summarize_missing()` function to check that you have removed all matching values. Assign the result to summary_after.
The helper functions are:

- `v_fun()`: This function takes an integer input and calculates the number of missing values in the vehicle column where the corresponding cause column is not missing.
- `c_fun()`: This function takes an integer input and calculates the number of missing values in the cause column where the corresponding vehicle column is not missing.
`summarize_missing()`: This function creates a summary data frame with the number of missing values in the vehicle and cause columns using the `v_fun()` and `c_fun()` functions.

```{r}
# The goal is to make sure only cells that contain NA for both vehicle and cause are filtered out
v_fun <- function(x){ 
    v_col <- paste('vehicle', x,  sep = "_" )
    c_col <- paste('cause_vehicle', x,  sep = "_" )
    sum(is.na(mvc[v_col]) & !is.na(mvc[c_col]))
}

c_fun <- function(x){ 
    v_col <- paste('vehicle', x,  sep = "_" )
    c_col <- paste('cause_vehicle', x,  sep = "_" )
    sum(!is.na(mvc[v_col]) & is.na(mvc[c_col]))
}

summarize_missing <- function(){
    
    col_labels  <-  c('v_number', 'vehicle_missing', 'cause_missing')
    
    v_na <- map_int(1:5, v_fun )
    c_na <- map_int(1:5, c_fun )
    
    vc_na_df  <-  tibble(1:5, v_na, c_na)
    colnames(vc_na_df) <- col_labels
    vc_na_df
}

summary_before  <-  summarize_missing() 

print(summary_before)

# for (x in 1:5 ){
#    v_col <- paste('vehicle', x,  sep = "_" )
#    c_col <- paste('cause_vehicle', x,  sep = "_" )
for (x in 1:5 ){
    v_col <- paste('vehicle', x,  sep = "_" )
    c_col <- paste('cause_vehicle', x,  sep = "_" )
    
    # create a logical vector for each column
    v_missing_logical  <-  is.na(mvc[v_col]) & !is.na(mvc[c_col])
    c_missing_logical  <-  !is.na(mvc[v_col]) & is.na(mvc[c_col])
    
    # replace the values matching the logical vector for each column
    mvc <- mvc %>%
        mutate_at(c(v_col), function(x) if_else(v_missing_logical,"Unspecified", x ))

    mvc <- mvc %>%
        mutate_at(c(c_col), function(x) if_else(c_missing_logical,"Unspecified", x ))
}

summary_after  <-  summarize_missing()
```


## Missing Data in the "Location" Columns

Let's view the work we've done across the past few screens by looking at the NA correlation plot for the last 10 columns:


```{r}
mvc_na_vehicle <- mvc_na %>% select(contains("vehicle"))
missing_vehicle_corr  <-  round(cor(mvc_na_vehicle), 2)

plot_na_correlation(missing_vehicle_corr)
```

You can see the perfect correlation between each pair of vehicle/cause columns represented by 
1.0
 in each square, which means that there is a perfect relationship between the five pairs of vehicle/cause columns.

Let's now turn our focus to the final set of columns that contain missing values — the columns that relate to the location of the accident. We'll start by looking at the first few rows to refamiliarize ourselves with the data:

```{r}
location_cols  <-  c('borough', 'location', 'on_street', 'off_street', 'cross_street')
location_data  <-  mvc[location_cols]
head(location_data)
```

Next, let's look at counts of the NA values in each column:

```{r}
colSums(is.na(location_data))
```


These columns have a lot of missing values! Keep in mind that all of these five columns represent the same thing — the location of the collision. We can potentially use the non-NA values to impute some of the NA values.

To see where we might be able to do this, let's look for correlations between the missing values:

```{r}
mvc_na_location <- mvc_na[location_cols]
missing_location_corr  <-  round(cor(mvc_na_location), 2)

plot_na_correlation(missing_location_corr)
```

None of these columns have strong correlations except for `off_street` and `on_street`, which have a near perfect negative correlation. That means for almost every row that has a NA value in one column, the other has a non-NA value and vice-versa.

The final way we'll look at the NA values in these columns is to plot a NA matrix, but we'll sort the data first using `arrange_all()` function from `dplyr` package. This will gather some of the NA and non-NA values together and make patterns more obvious:

```{r}
mvc_na_location <- mvc_na[location_cols] %>% arrange_all(desc)

plot_na_matrix(mvc_na_location)
```

Let's make some observations about the missing values across these columns:

1. About two-thirds of rows have non-NA values for `borough`, but of the values that are missing, most have non-NA values for `location` and one or more of the street name columns.
2. Less than one-tenth of rows have missing values in the `location` column, but most of these have non-NA values in one or more of the street name columns.
3. Most rows have a non-NA value for either `on_street` or `off_street`, and some also have a value for `cross_street`.

Combined, this means that we will be able to impute a lot of the missing values by using the other columns in each row. To do this, we can use geolocation APIs that take either an address or location coordinates, and return information about that location.

Because the focus of this lesson is working with missing data, we have pre-prepared supplemental data using APIs. On the next screen, we'll learn more about how that data was prepared and then use it to fill in missing values.

## Imputing Location Data

We prepared the supplemental data using the GeoPy package, which makes working with Geocoding APIs like the Google Maps API easier. Here's the strategy we used to prepare the supplemental data:

- For rows with `location` values but missing values in either `borough` or the street name columns, we used geocoding APIs to look up the `location` coordinates to find the missing data.
- For rows with values in the street name columns missing `borough` and/or `location` data, we used geocoding APIs to look up the address to find the missing data.
You can learn more about working with APIs in our APIs and Web Scraping course.

The supplemental data is in a CSV called supplemental_data.csv; let's read this into a dataframe and familiarize ourself with the data:

```{r}
sup_data  <-  read_csv('supplemental_data.csv')
head(sup_data)
```
The supplemental data has five columns from our original dataset — the `unique_key` that identifies each collision, and four of the five location columns. The `cross_street` column is not included because the geocoding APIs we used don't include data on the nearest cross street to any single location.

Let's take a look at a NA matrix for the supplemental data:



```{r}
sup_data_na <- map_df(sup_data, function(x) as.numeric(is.na(x)))

plot_na_matrix(sup_data_na)
```

[Apart from the `unique_key` column, you'll notice that there are a lot more missing values than our main dataset. This makes sense, as we didn't prepare supplemental data where the original dataset had non-NA values.]{color="red"}

If the `unique_key` column in both the original and supplemental data has the same values in the same order, we'll be able to use `mutate()` and `if_else()` function to add our supplemental data to our original data. We can check this by using `setequal()` function:



```{r}
mvc_keys  <-  mvc['unique_key']
sup_keys  <-  sup_data['unique_key']

set_equal <- setequal(mvc_keys, sup_keys)

print(set_equal)
```
Note that `setequal()` can return `TRUE` even if the values are in a different order. On the other hand, `identical()` ensures that the same values are in the same order.

Now that we've verified the data, it's time to use it to impute missing values.


We read the supplemental data into a dataframe called `sup_data.` Additionally, we provided a vector of the location columns, `location_cols`, and calculated the number of NA values in these columns.

1. Loop over the column names in `location_cols`. In each iteration of the loop, replace values in the column in the `mvc` dataframe:
- The logical vector should represent whether the values in column in the `mvc` has a NA value or not.
- Where the logical vector is true, the value should be replaced with the equivalent value in `sup_data`.
2. Calculate the number of NA values across the `location_cols` columns in `mvc` after you adding the supplemental data. Assign the result to `na_after`.

```{r}
v_fun <- function(x){ 
    v_col <- paste('vehicle', x,  sep = "_" )
    c_col <- paste('cause_vehicle', x,  sep = "_" )
    sum(is.na(mvc[v_col]) & !is.na(mvc[c_col]))
}

c_fun <- function(x){ 
    v_col <- paste('vehicle', x,  sep = "_" )
    c_col <- paste('cause_vehicle', x,  sep = "_" )
    sum(!is.na(mvc[v_col]) & is.na(mvc[c_col]))
}

summarize_missing <- function(){
  
    col_labels  <-  c('v_number', 'vehicle_missing', 'cause_missing')
    
    v_na <- map_int(1:5, v_fun )
    c_na <- map_int(1:5, c_fun )
    
    vc_na_df  <-  tibble(1:5, v_na, c_na)
    colnames(vc_na_df) <- col_labels
    vc_na_df
}

summary_before  <-  summarize_missing() 

# for (x in 1:5 ){
#    v_col <- paste('vehicle', x,  sep = "_" )
#    c_col <- paste('cause_vehicle', x,  sep = "_" )


location_cols  <-  c('location', 'on_street', 'off_street', 'borough')
na_before  <-  colSums(is.na(mvc[location_cols]))
for (x in 1:5 ){
    v_col <- paste('vehicle', x,  sep = "_" )
    c_col <- paste('cause_vehicle', x,  sep = "_" )
    
    # create a logical vector for each column
    v_missing_logical  <-  is.na(mvc[v_col]) & !is.na(mvc[c_col])
    c_missing_logical  <-  !is.na(mvc[v_col]) & is.na(mvc[c_col])
    
    # replace the values matching the logical vector for each column
    mvc <- mvc %>%
        mutate_at(c(v_col), function(x) if_else(v_missing_logical,"Unspecified", x ))

    mvc <- mvc %>%
        mutate_at(c(c_col), function(x) if_else(c_missing_logical,"Unspecified", x ))
}

summary_after  <-  summarize_missing()

for (col in location_cols ) {
    mvc[is.na(mvc[col]),col] <- sup_data[is.na(mvc[col]),col]
}

na_after  <-  colSums(is.na(mvc[location_cols]))

print(na_after)
```
























